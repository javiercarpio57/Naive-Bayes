---
title: "Modelos de Regresión Lineal"
author: "Javier Carpio & Paul Belches"
date: "25/3/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
library(rpart)
library(caret)
library(tree)
library(rpart.plot)
library(randomForest)
library(plyr)
library(dplyr) 
library(fpc)
library(corrplot)
library(e1071)
```

## Naive Bayes

Como primer paso para realizar el método de Naive Bayes se procederá con realizar el proceso de clustering que nos permite agrupar las casas en Bajo, Intermedio y Alto, y así obtener la variable respuesta:


```{r}
datos<-read.csv("train.csv")

#data <-select(datos, GarageYrBlt, GrLivArea, X1stFlrSF, X2ndFlrSF, GarageCars, SalePrice)
#data <- na.omit(data)
data <-select(datos, LotFrontage, LotArea, YearBuilt, YearRemodAdd, MasVnrArea, BsmtFinSF1,BsmtFinSF2,BsmtUnfSF,TotalBsmtSF,X1stFlrSF,X2ndFlrSF,LowQualFinSF,GrLivArea,TotRmsAbvGrd,Fireplaces,GarageYrBlt,GarageCars,GarageArea,WoodDeckSF,OpenPorchSF,EnclosedPorch,ScreenPorch,PoolArea,MoSold,YrSold,SalePrice)
#Data cleanup
data <- na.omit(data)

```

Además se realizar una matriz de correlación, para ver el peso de las variables en el modelo. 

```{r}
matriz_cor <- cor(data)
matriz_cor
#corrplot(matriz_cor)
```

Tomando en cuenta el bajo valor de correlaciónse eliminan todas las variables con un valor menor a 0.6, del modelo.

Se grafican las variables en pares para observar la relación que puede existir entre las mismas, además de realizar un grafico calor.

```{r}
data <- select(datos, TotalBsmtSF,X1stFlrSF,GrLivArea,GarageCars,GarageArea, SalePrice)
data <- na.omit(data)
plot(data)
matriz_cor <- cor(data)
matriz_cor
corrplot(matriz_cor)
```

Por el alto valor de correlación entre TotalBsmtSF y X1stFlrSF. Se elimina X1stFlrSF, por tener menor valor de correlación contra SalePrice. De igual manera se elimina GarageArea. 

```{r}
data <- select(datos, TotalBsmtSF,GrLivArea,GarageCars, SalePrice)
data <- na.omit(data)
```


Se procede a realizar un test de normalidad para las variables no categoricas.

```{r}
hist(data$GrLivArea)
hist(data$TotalBsmtSF)
```

```{r}
library(normtest) 
library(nortest) 
sf.test(data$TotalBsmtSF)
sf.test(data$GrLivArea)
```

Utilizando el test de normalidad de Shapiro-Francia, podemos afirmar que las varibales mostradas anteriormente cuentan con una distribución normal. 

A continuación se procede a la generación de los clústeres mediante K-means. 

```{r}
data <- select(datos, TotalBsmtSF,GrLivArea,GarageCars, SalePrice)
data <- na.omit(data)
cluster <- data
km<-kmeans(data, 3)
data$grupo<-km$cluster

g1 <- data[data$grupo==1, ]
g2 <- data[data$grupo==2, ]
g3 <- data[data$grupo==3, ]

summary(g1$SalePrice) 
summary(g2$SalePrice) 
summary(g3$SalePrice) 
```

```{r}
data$grupo <- mapvalues(data$grupo, c(1,2,3), c("Bajo", "Alto", "Intermedio")) #El orden del identificador depende de los resumentes de la celda anterior
```

Como se puede observar se genera una clasificación adecuada de grupos,Intermedio,Alto y Bajo. 

Particionamos el dataset en test y train, pero poder realizar un proceso de entrenamiento (con train) y verificar el modelo (con test)
```{r}
porcentaje<-0.7
set.seed(123)

corte <- sample(nrow(data), nrow(data) * porcentaje)
train <- data[corte, ]
test <- data[-corte, ]
```

Ahora sí... creamos el modelo de Naive Bayes y observemos cómo rinde el modelo con la matriz de confusión:

```{r}
head(test)
```



```{r}
modelo<-naiveBayes(as.factor(train$grupo)~., data=train)
predBayes<-predict(modelo, newdata = test[, 1:4])
confusionMatrix(table(predBayes, test$grupo))
```

La sensitividad y la especificidad son bastantes altas, pues, están por encima de 0.94 en promedio, por lo tanto, podemos decir que el Naive Bayes está acertando. Además, hay un accuracy del 93.85%, así que podemos asumir que no hay overfitting porque se ajustó bastante bien con el dataset de test. También, se confirma que el modelo está bastante bien pues en la matriz de confusión:

      * Las casas de alto precio se acertaron 33 de 40 = 83%.
      * Las casas de precio bajo se acertaron 219 de 226 = 98%.
      * Las casas de precio intermedio se certaron 158 de 173 = 91%.
      
Ahora, lo haremos con Cross Validation (con Caret):
```{r echo=FALSE, warning=FALSE, message=FALSE}
ct <- trainControl(method = "cv", train[, 1:4], number=10, verboseIter=T)
modeloCaret <- train(as.factor(grupo)~., data=data, method="nb", trControl = ct)
```

```{r echo=FALSE, warning=FALSE, message=FALSE}
prediccionCaret <- predict(modeloCaret, newdata = test[, 1:4])
```
```{r}
confusionMatrix(table(prediccionCaret, test$grupo))
```
      
En el proceso de Cross Validation, notamos que es ligeramente más certero que Naive Bayes, debido a:

      * Las casas de alto precio se acertaron 35 de 37 = 95%.
      * Las casas de precio bajo se acertaron 222 de 225 = 99%.
      * Las casas de precio intermedio se certaron 167 de 177 = 94%.
      
Todos están arriba del proceso anterior, además, se obtuvo un 0.95 de accuracy y el promedio de la sensitividad y la especificidad es del 0.96.

## Árbol de clasificación
      
```{r  warning=FALSE}

#Clasiffication Tree
dt_model<-rpart(train$grupo~.,train[1:3],method = "class") #Genrar modelo
rpart.plot(dt_model)
prediccionCT <- predict(dt_model, newdata = test[, 1:3]) #Predecir
#prediccionCT
columnaMasAlta<-apply(prediccionCT, 1, function(x) colnames(prediccionCT)[which.max(x)])
test$prediccionCT<-columnaMasAlta
confusionMatrix(table(test$prediccionCT,test$grupo))
```

Como se puede observar el árbol de clasificación tiene un accuracy del 81.55%, así que podemos asumir que no hay overfitting porque se ajustó bastante bien con el dataset de test. A partir de la matriz de confusión podemos observar los siguintes valores. 

      * Las casas de alto precio se acertaron 47 de 28 = 60%.
      * Las casas de precio bajo se acertaron 228 de 209 = 92%.
      * Las casas de precio intermedio se certaron 164 de 121 = 74%.

## Conclusión

Podemos afirmar que el algoritmo de Naive Bayes, es superior a el árbol de clasificación. Esto gracias a que al comparar factores como el accuracy y la sensitividad y especificidad son mejores. Además los porcentajes de predicción para cada uno de los grupos también es mejor. En el caso de el árbol de clasificación, el hecho de que tenga un porcentaje del 60% para las casas de alto precio, puede llegar a ser una desventaja. Gracias a que puede significar una perdida monetaria. Por lo que se recomienda la utlización de naive bayes en su lugar. 




